{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation of the song metadata file, the track_id is encoded as numerical values to save space.\n",
    "\n",
    "# import numpy as np\n",
    "# #import pylab as Plot\n",
    "# import pandas as pd\n",
    "# import gc\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "#from sklearn.externals import joblib\n",
    "import joblib #direct import working on my machine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hickle as hkl\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#data_path = 'D:/skip spotify code/track_features/'\n",
    "#data_path = '/home/sc4/skip/track_features/'\n",
    "# data_path = 'Skip_Data/'\n",
    "data_path = 'C:/Users/13528/Task 1/Skip Data/'\n",
    "\n",
    "song_fea_0 = pd.read_csv(data_path+'tf_000000000000.csv')\n",
    "song_fea_1 = pd.read_csv(data_path+'tf_000000000001.csv')\n",
    "\n",
    "song_fea_0 = pd.concat((song_fea_0,song_fea_1))\n",
    "\n",
    "le = LabelEncoder()\n",
    "#\n",
    "song_fea_0['track_id'] = le.fit_transform(song_fea_0['track_id'])\n",
    "\n",
    "joblib.dump(le, 'le_track_id.pkl')\n",
    "\n",
    "song_fea_0['mode'] = le.fit_transform(song_fea_0['mode'])\n",
    "\n",
    "song_fea_0.to_parquet('spotify_song_fea.parquet')\n",
    "\n",
    "song_fea_1 = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "0 0.1\n",
      "0 0.2\n",
      "0 0.3\n",
      "0 0.4\n",
      "0 0.5\n",
      "0 0.6\n",
      "0 0.7\n",
      "0 0.8\n",
      "0 0.9\n"
     ]
    }
   ],
   "source": [
    "# Conversion of the training session csv files to parquet files for smaller size and fast loading, the track_ids and session ids are encoded as numerical values to save space. The generated files are saved in the 'Skip_Data/' folder.\n",
    "# If done correcly, the name of the generated parquet files should be something like \"log_3_20180918_000000000000.csv.parquet\"\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "# data_path = 'Skip_Data/'\n",
    "data_path = 'C:/Users/13528/Task 1/Skip Data/'\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# dirs = os.listdir( 'train_fold/' )\n",
    "dirs = os.listdir('C:/Users/13528/Task 1/train_fold/')\n",
    "count = 0\n",
    "for json_file in dirs:    \n",
    "    print(0, count/len(dirs))\n",
    "    \n",
    "#     train_data = pd.read_csv('train_fold/'+json_file)\n",
    "    train_data = pd.read_csv('C:/Users/13528/Task 1/train_fold/'+json_file)\n",
    "    \n",
    "    le_session = LabelEncoder()\n",
    "\n",
    "    train_data['session_id'] = le_session.fit_transform(train_data['session_id'])\n",
    "\n",
    "    train_data['track_id_clean'] = le_track_id.transform(train_data['track_id_clean'])\n",
    "\n",
    "    train_data.to_parquet(data_path+json_file+'.parquet')\n",
    "     \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# Conversion of the training session csv files to parquet files for smaller size and fast loading, the track_ids and session ids are encoded as numerical values to save space. The generated files are saved in the 'Skip_Data/' folder.\n",
    "# There are two kinds of test files, namely files with names that start with \"log_input\" or \"log_prehistory\". If this part of code is executed correctly, the names of the generated parquet files should be correspondingly be something like \"pred_20180718.parquet\" and \"prehist_20180810.parquet\" for \n",
    "\n",
    "\n",
    "import hickle as hkl\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "# test_files = glob.glob('test_fold/log_input_*_000000000000.csv')\n",
    "test_files = glob.glob('C:/Users/13528/Task 1/test_fold/log_input_*_000000000000.csv')\n",
    "\n",
    "for i in tqdm(range(len(test_files))):\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    file = pd.read_csv(test_files[i])\n",
    "    \n",
    "#     prefix = 'test_fold/log_prehistory'\n",
    "    prefix = 'C:/Users/13528/Task 1/test_fold/log_prehistory'\n",
    "    \n",
    "#     pre_file = pd.read_csv(prefix + test_files[i][33:])\n",
    "    pre_file = pd.read_csv(prefix + test_files[i][41:])\n",
    "    \n",
    "    file['session_id'] = le.fit_transform(file['session_id'])\n",
    "    \n",
    "    pre_file['session_id'] = le.transform(pre_file['session_id'])\n",
    "    \n",
    "    pre_file['track_id_clean'] = le_track_id.transform(pre_file['track_id_clean'])\n",
    "    file['track_id_clean'] = le_track_id.transform(file['track_id_clean'])\n",
    "    \n",
    "#     file.to_parquet('test_pred/'+ 'pred_'+test_files[i][34:42]+'.parquet')\n",
    "    \n",
    "#     pre_file.to_parquet('test_pred/'+ 'prehist_'+test_files[i][34:42]+'.parquet')\n",
    "    file.to_parquet('C:/Users/13528/Task 1/test_pred/'+ 'pred_'+test_files[i][42:50]+'.parquet')\n",
    "    \n",
    "    pre_file.to_parquet('C:/Users/13528/Task 1/test_pred/'+ 'prehist_'+test_files[i][42:50]+'.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13528\\AppData\\Local\\Temp/ipykernel_18508/521039522.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  raw_data = raw_data.astype(np.int)\n",
      "C:\\Users\\13528\\AppData\\Local\\Temp/ipykernel_18508/521039522.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data[raw_data[:,0].astype(np.int)*30+raw_data[:,1].astype(np.int)-1] = raw_data[:,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1\n",
      "0 0.2\n",
      "0 0.3\n",
      "0 0.4\n",
      "0 0.5\n",
      "0 0.6\n",
      "0 0.7\n",
      "0 0.8\n",
      "0 0.9\n"
     ]
    }
   ],
   "source": [
    "# Preparation of the file for Glove training.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# data_path = 'Skip_Data/'\n",
    "data_path = 'C:/Users/13528/Task 1/Skip Data/'\n",
    "\n",
    "train_files = glob.glob(data_path + '*.csv.parquet')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "#data_path = 'D:/skip spotify code/track_features/'\n",
    "#data_path = '/home/sc4/skip/track_features/'\n",
    "\n",
    "le_track_id = joblib.load('le_track_id.pkl')\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "#dirs = dirs[0:200]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "count = 0\n",
    "for file in train_files:    \n",
    "    print(0, count/len(train_files))\n",
    "    \n",
    "    train_data = pd.read_parquet(file)\n",
    "    \n",
    "    cols = ['session_id', 'session_position', 'session_length', 'track_id_clean']\n",
    "    \n",
    "    train_data = train_data[cols]\n",
    "    \n",
    "    train_data['track_id_clean'] = train_data['track_id_clean'] + 1\n",
    "    \n",
    "    raw_data = np.array(train_data.values)*1\n",
    "\n",
    "    raw_data = raw_data.astype(np.int)    \n",
    "    \n",
    "    n_session = np.max(train_data['session_id'])+1\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    data = np.zeros((n_session*30))\n",
    "    \n",
    "    data[raw_data[:,0].astype(np.int)*30+raw_data[:,1].astype(np.int)-1] = raw_data[:,3]\n",
    "\n",
    "    if count == 0:\n",
    "        all_data = data\n",
    "    else:\n",
    "        all_data = np.concatenate((all_data,data))\n",
    "     \n",
    "    count = count + 1\n",
    "\n",
    "np.savetxt('glove_data.txt',all_data,newline=' ', delimiter = ' ', fmt='%i') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58950990"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After learning the Glove embedding, let the embedding txt file be named as 'vectors_150.txt', this part of code convert the txt file to numpy format.\n",
    "\n",
    "import numpy as np\n",
    "import pylab as Plot\n",
    "import pandas as pd\n",
    "import gc\n",
    "import hickle as hkl\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_coefs(word,*arr): return (word), np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('vectors_150.txt'))\n",
    "\n",
    "song_embedding_matrix = np.zeros((3706389,150))\n",
    "# song_embedding_matrix = np.zeros((393006,150))\n",
    "\n",
    "keys = embeddings_index.keys()\n",
    "\n",
    "for i in range(0,song_embedding_matrix.shape[0]):\n",
    "        tmp = embeddings_index.get(str(i))\n",
    "        if tmp is not None:\n",
    "            song_embedding_matrix[i,:] = tmp\n",
    "            \n",
    "# hkl.dump(song_embedding_matrix, 'song_embedding_matrix_150.hkl', mode='w', compression='gzip')\n",
    "hkl.dump(song_embedding_matrix, 'song_embedding_matrix_150.hkl', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song_embedding_matrix_150.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(song_embedding_matrix, 'song_embedding_matrix_150.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3706389, 150)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spotify_song_array_glove = hkl.load('song_embedding_matrix_150.hkl')\n",
    "spotify_song_array_glove = joblib.load('song_embedding_matrix_150.pkl')\n",
    "# os.path.getsize('song_embedding_matrix_150.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30616799, -0.61709601, -0.22131801, ..., -0.30539101,\n",
       "         1.09236097,  1.19929302],\n",
       "       [ 0.30116001, -0.171077  , -0.110602  , ...,  0.26418701,\n",
       "        -0.17385501, -0.245979  ],\n",
       "       [-0.92715001, -0.61823601, -0.028583  , ..., -0.61596602,\n",
       "        -0.52215099,  0.49527901],\n",
       "       ...,\n",
       "       [-0.14865901,  0.071279  , -0.24946199, ...,  0.026126  ,\n",
       "        -0.008469  ,  0.036255  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.22290801,  0.156873  , -0.325515  , ..., -0.051759  ,\n",
       "        -0.086463  , -0.54713899]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_song_array_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.306168, -0.617096, -0.221318,  0.620467,  0.251854, -0.269983,\n",
       "       -0.814901,  0.220307,  0.232154, -0.834252,  0.046133,  0.463726,\n",
       "       -1.259767,  0.260635, -0.068549, -0.817818, -1.242769,  0.193992,\n",
       "       -0.022523,  0.139119, -0.809364, -0.015667, -0.024639,  0.226955,\n",
       "       -0.223547, -0.475842, -0.34594 , -0.319357, -0.12704 ,  0.121938,\n",
       "       -0.028495, -0.949348,  0.266388, -0.547475,  0.094223, -0.419741,\n",
       "       -0.989508,  0.190096, -0.488746, -0.256873, -0.275853,  0.725631,\n",
       "        4.279365, -0.123002, -0.129986, -1.528008,  0.531471, -0.029998,\n",
       "        0.432335,  0.702673,  1.26482 , -0.477329,  0.323498, -0.343377,\n",
       "       -0.244294,  0.680409,  0.684785, -0.297188,  0.211914, -0.342224,\n",
       "        0.023286, -0.433466,  1.286423, -0.709696,  0.125279, -0.594115,\n",
       "        0.639779,  0.365565,  0.405594, -0.081768, -0.186266,  0.054931,\n",
       "       -0.678648,  1.156425, -1.190904, -0.108001,  0.302235,  0.594203,\n",
       "       -0.05948 ,  0.036465, -0.922106,  0.210533,  0.473558,  0.960569,\n",
       "       -0.421757, -0.812555,  0.740121, -0.122989,  0.647803, -0.279981,\n",
       "       -0.053762, -0.276678, -1.330097,  0.578745, -0.008476,  0.096385,\n",
       "        0.264415, -0.542401,  0.679012, -0.359197, -0.141185, -0.860484,\n",
       "       -0.649337, -0.117275, -0.448174, -1.072045,  0.342461,  0.296928,\n",
       "       -0.2388  ,  0.269303,  0.048171,  0.265632,  0.700115, -0.481074,\n",
       "        0.120199, -0.372181,  0.321249, -0.867964,  0.076392, -0.127908,\n",
       "       -0.222742,  0.211381,  0.246139,  1.043738, -0.744383, -2.255595,\n",
       "       -2.782966, -0.581144, -0.404946, -0.109498, -0.333825, -0.325805,\n",
       "       -0.447606,  0.31193 , -0.386456, -1.513917, -0.449435,  1.873541,\n",
       "        0.734869, -0.916956, -0.093304, -0.153542,  0.099857,  0.617896,\n",
       "       -0.707598, -0.361359, -1.454082, -0.305391,  1.092361,  1.199293],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
